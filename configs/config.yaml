# BSI-Agent Configuration

# Data paths
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  dialogues_dir: "data/synthetic_dialogues"

# Model configuration
model:
  base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
  # Alternative: "mistralai/Mistral-7B-Instruct-v0.3"

  # LoRA configuration
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

  # Quantization (QLoRA)
  quantization:
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"

# Training configuration
training:
  num_epochs: 5
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  warmup_ratio: 0.1
  max_seq_length: 4096
  output_dir: "outputs/model"

# Dialogue generation
dialogue_generation:
  model: "gpt-4o"  # or "gpt-4-turbo"
  num_dialogues: 500
  max_turns: 10
  temperature: 0.7

# Environment configuration
environment:
  max_context_tokens: 2048
  include_antibiogram: true
  include_guidelines: true

# Evaluation
evaluation:
  top_k: [1, 3, 5]
  compute_brier_score: true
  compute_grounding_score: true

# Safety guardrails
guardrails:
  check_allergies: true
  check_contraindications: true
  allergy_database: "configs/allergies.yaml"
